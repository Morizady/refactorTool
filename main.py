#!/usr/bin/env python3
"""
Javaé¡¹ç›®æ¥å£åˆ†æå·¥å…· - ä¸“æ³¨äºSpring Booté¡¹ç›®åˆ†æ
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
import argparse

from endpoint_extractor import EndpointExtractor
from equivalence_matcher import EquivalenceMatcher
from call_chain_analyzer import CallChainAnalyzer
from sql_mapper_analyzer import SQLMapperAnalyzer
from ai_generator import AIGenerator
from ast_deep_call_chain_analyzer import ASTDeepCallChainAnalyzer
from deep_call_chain_analyzer import DeepCallChainAnalyzer
from jdt_call_chain_analyzer import JDTDeepCallChainAnalyzer

def generate_call_tree(endpoint_path: str, output_dir: str = "./migration_output", parse_method: str = "regex", max_depth: int = 4):
    """ç”ŸæˆæŒ‡å®šæ¥å£çš„æ·±åº¦è°ƒç”¨é“¾æ ‘"""
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆè°ƒç”¨é“¾æ ‘: {endpoint_path}")
    print(f"ğŸ“Š è§£ææ–¹æ³•: {parse_method.upper()}")
    print(f"ğŸ“ æœ€å¤§æ·±åº¦: {max_depth}")
    
    analysis_file = f"{output_dir}/endpoint_analysis.json"
    
    if not os.path.exists(analysis_file):
        print(f"âŒ åˆ†ææ–‡ä»¶ä¸å­˜åœ¨: {analysis_file}")
        print("è¯·å…ˆè¿è¡Œå•é¡¹ç›®åˆ†æç”Ÿæˆåˆ†ææ•°æ®ï¼š")
        print("python main.py --single /path/to/project")
        return
    
    # åŠ è½½åˆ†ææ•°æ®
    print("ğŸ“‚ æ­£åœ¨åŠ è½½åˆ†ææ•°æ®...")
    try:
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(analysis_data)} ä¸ªæ¥å£çš„åˆ†ææ•°æ®")
    except Exception as e:
        print(f"âŒ è¯»å–åˆ†ææ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ¥å£
    print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾åŒ¹é…çš„æ¥å£: {endpoint_path}")
    matching_endpoints = []
    for endpoint_data in analysis_data:
        endpoint = endpoint_data['endpoint']
        if endpoint_path in endpoint['path'] or endpoint_path == endpoint['path']:
            matching_endpoints.append(endpoint_data)
    
    if not matching_endpoints:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ¥å£: {endpoint_path}")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(matching_endpoints)} ä¸ªåŒ¹é…çš„æ¥å£")
    
    # é€‰æ‹©æ¥å£
    if len(matching_endpoints) > 1:
        print(f"ğŸ” æ‰¾åˆ° {len(matching_endpoints)} ä¸ªåŒ¹é…çš„æ¥å£:")
        for i, endpoint_data in enumerate(matching_endpoints, 1):
            endpoint = endpoint_data['endpoint']
            print(f"{i}. {endpoint['method']} {endpoint['path']} - {endpoint['name']}")
        
        try:
            choice = int(input("\nè¯·é€‰æ‹©è¦åˆ†æçš„æ¥å£ (è¾“å…¥åºå·): ")) - 1
            if 0 <= choice < len(matching_endpoints):
                selected_endpoint = matching_endpoints[choice]
            else:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©")
                return
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            return
    else:
        selected_endpoint = matching_endpoints[0]
    
    # ç”Ÿæˆè°ƒç”¨æ ‘
    print("ğŸŒ³ å¼€å§‹ç”Ÿæˆæ·±åº¦è°ƒç”¨é“¾æ ‘...")
    
    # ä½¿ç”¨æ–°çš„JDTæ·±åº¦åˆ†æå™¨
    if parse_method == "jdt":
        _generate_jdt_call_tree(selected_endpoint, output_dir, max_depth)
    else:
        _generate_call_tree_md(selected_endpoint, output_dir, parse_method, max_depth)

def _generate_jdt_call_tree(endpoint_data: Dict, output_dir: str, max_depth: int = 6):
    """ä½¿ç”¨JDTç”Ÿæˆæ·±åº¦è°ƒç”¨æ ‘"""
    endpoint = endpoint_data['endpoint']
    
    # ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
    file_path = endpoint['file_path']
    project_root = None
    
    print("ğŸ“ æ­£åœ¨ç¡®å®šé¡¹ç›®æ ¹ç›®å½•...")
    # å°è¯•æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
    path_parts = file_path.split(os.sep)
    for i, part in enumerate(path_parts):
        if part == 'src':
            # æ‰¾åˆ°srcç›®å½•ï¼Œé¡¹ç›®æ ¹ç›®å½•å°±æ˜¯srcçš„ä¸Šä¸€çº§
            project_root = os.sep.join(path_parts[:i])
            break
    
    if not project_root:
        # å¦‚æœæ²¡æ‰¾åˆ°srcç›®å½•ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        for i, part in enumerate(path_parts):
            if part in ['main', 'java']:
                project_root = os.sep.join(path_parts[:max(0, i-2)])
                break
    
    if not project_root:
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        project_root = os.path.dirname(os.path.dirname(file_path))
    
    # ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•å­˜åœ¨
    if not os.path.exists(project_root):
        project_root = os.path.dirname(file_path)
    
    print(f"ğŸ—ï¸ å¼€å§‹JDTæ·±åº¦åˆ†ææ¥å£: {endpoint['name']}")
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ“Š ä½¿ç”¨è§£ææ–¹æ³•: JDT")
    
    try:
        # åˆå§‹åŒ–JDTæ·±åº¦åˆ†æå™¨
        from jdt_call_chain_analyzer import JDTDeepCallChainAnalyzer
        analyzer = JDTDeepCallChainAnalyzer(project_root)
        
        # åˆ†ææ·±åº¦è°ƒç”¨æ ‘
        print(f"ğŸš€ å¼€å§‹åˆ†æä¸»æ–¹æ³•: {endpoint['handler']}")
        print("=" * 60)
        
        call_tree = analyzer.analyze_deep_call_tree(
            file_path, 
            endpoint['handler'],
            max_depth=max_depth
        )
        
        print("=" * 60)
        
        if call_tree:
            # ç”ŸæˆæŠ¥å‘Š
            print("ğŸ“ æ­£åœ¨ç”Ÿæˆæ·±åº¦è°ƒç”¨æ ‘æŠ¥å‘Š...")
            endpoint_path = f"{endpoint['method']} {endpoint['path']}"
            report_file = analyzer.generate_call_tree_report(call_tree, endpoint_path, output_dir)
            
            print(f"âœ… JDTæ·±åº¦è°ƒç”¨æ ‘å·²ç”Ÿæˆ: {report_file}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            total_calls = analyzer._count_total_calls(call_tree)
            max_depth_actual = analyzer._get_max_depth(call_tree)
            unique_classes = analyzer._count_unique_classes(call_tree)
            mapping_count = len(analyzer.method_mappings)
            
            print(f"ğŸ“Š åˆ†æç»Ÿè®¡:")
            print(f"  - è§£ææ–¹æ³•: JDT (Eclipse JDT)")
            print(f"  - æ€»è°ƒç”¨æ•°: {total_calls}")
            print(f"  - æœ€å¤§æ·±åº¦: {max_depth_actual}")
            print(f"  - æ¶‰åŠç±»æ•°: {unique_classes}")
            print(f"  - æ–¹æ³•æ˜ å°„æ•°: {mapping_count}")
            
            # æ˜¾ç¤ºéƒ¨åˆ†æ–¹æ³•æ˜ å°„ç¤ºä¾‹
            if analyzer.method_mappings:
                print(f"\nğŸ“‹ æ–¹æ³•æ˜ å°„ç¤ºä¾‹:")
                for i, mapping in enumerate(analyzer.method_mappings[:3], 1):
                    print(f"  {i}. {mapping.interface_call} -> {mapping.implementation_call}")
                if len(analyzer.method_mappings) > 3:
                    print(f"  ... è¿˜æœ‰ {len(analyzer.method_mappings) - 3} ä¸ªæ˜ å°„")
        else:
            print("âŒ JDTæ·±åº¦è°ƒç”¨æ ‘åˆ†æå¤±è´¥")
        
        # å…³é—­åˆ†æå™¨
        analyzer.shutdown()
        
    except Exception as e:
        print(f"âŒ JDTåˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def _generate_call_tree_md(endpoint_data: Dict, output_dir: str, parse_method: str = "regex", max_depth: int = 4):
    """ç”Ÿæˆè°ƒç”¨æ ‘çš„Markdownæ–‡ä»¶"""
    endpoint = endpoint_data['endpoint']
    call_chain = endpoint_data['call_chain']
    
    # ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
    file_path = endpoint['file_path']
    project_root = None
    
    print("ğŸ“ æ­£åœ¨ç¡®å®šé¡¹ç›®æ ¹ç›®å½•...")
    # å°è¯•æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
    path_parts = file_path.split(os.sep)
    for i, part in enumerate(path_parts):
        if part == 'src':
            # æ‰¾åˆ°srcç›®å½•ï¼Œé¡¹ç›®æ ¹ç›®å½•å°±æ˜¯srcçš„ä¸Šä¸€çº§
            project_root = os.sep.join(path_parts[:i])
            break
    
    if not project_root:
        # å¦‚æœæ²¡æ‰¾åˆ°srcç›®å½•ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        for i, part in enumerate(path_parts):
            if part in ['main', 'java']:
                project_root = os.sep.join(path_parts[:max(0, i-2)])
                break
    
    if not project_root:
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        project_root = os.path.dirname(os.path.dirname(file_path))
    
    # ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•å­˜åœ¨
    if not os.path.exists(project_root):
        project_root = os.path.dirname(file_path)
    
    print(f"ï¿½ å¼€å§‹æ·±åº¦åˆ†:ææ¥å£: {endpoint['name']}")
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ“Š ä½¿ç”¨è§£ææ–¹æ³•: {parse_method.upper()}")
    
    # æ ¹æ®è§£ææ–¹æ³•é€‰æ‹©åˆ†æå™¨
    if parse_method == "jdt":
        print("ğŸ—ï¸  æ­£åœ¨åˆå§‹åŒ–JDTæ·±åº¦åˆ†æå™¨...")
        analyzer = JDTDeepCallChainAnalyzer(project_root)
    elif parse_method == "ast":
        print("ğŸ—ï¸  æ­£åœ¨åˆå§‹åŒ–ASTæ·±åº¦åˆ†æå™¨...")
        analyzer = ASTDeepCallChainAnalyzer(project_root)
    else:
        print("ğŸ—ï¸  æ­£åœ¨åˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æ·±åº¦åˆ†æå™¨...")
        analyzer = DeepCallChainAnalyzer(project_root)
    
    # åˆ†æä¸»æ–¹æ³•
    print(f"ğŸš€ å¼€å§‹åˆ†æä¸»æ–¹æ³•: {endpoint['handler']}")
    print("=" * 60)
    main_analysis = analyzer.analyze_method_calls(
        file_path, 
        endpoint['handler'],
        max_depth=max_depth
    )
    print("=" * 60)
    
    # ç”ŸæˆMarkdownå†…å®¹
    print("ğŸ“ æ­£åœ¨ç”ŸæˆMarkdownå†…å®¹...")
    md_content = _build_call_tree_markdown(endpoint, call_chain, main_analysis, parse_method)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    method_suffix = "_ast" if parse_method == "ast" else "_regex"
    output_file = f"{output_dir}/call_tree_{endpoint['handler']}{method_suffix}.md"
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"âœ… è°ƒç”¨æ ‘å·²ç”Ÿæˆ: {output_file}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    total_calls = _count_total_calls_enhanced(main_analysis.get('calls', []))
    max_depth = _get_max_depth_enhanced(main_analysis.get('calls', []))
    interface_count = _count_interface_implementations(main_analysis.get('calls', []))
    
    print(f"ğŸ“Š åˆ†æç»Ÿè®¡:")
    print(f"  - è§£ææ–¹æ³•: {parse_method.upper()}")
    print(f"  - æ€»è°ƒç”¨æ•°: {total_calls}")
    print(f"  - æœ€å¤§æ·±åº¦: {max_depth}")
    print(f"  - æ¥å£å®ç°æ•°: {interface_count}")
    if hasattr(analyzer, 'analyzed_methods'):
        print(f"  - å·²åˆ†ææ–¹æ³•æ•°: {len(analyzer.analyzed_methods)}")

def _build_call_tree_markdown(endpoint: Dict, call_chain: Dict, deep_analysis: Dict, parse_method: str = "regex") -> str:
    """æ„å»ºè°ƒç”¨æ ‘çš„Markdownå†…å®¹ - å¢å¼ºç‰ˆ"""
    lines = []
    
    # æ ‡é¢˜
    lines.append(f"# {endpoint['name']} æ·±åº¦è°ƒç”¨é“¾åˆ†æ")
    lines.append("")
    
    # åŸºæœ¬ä¿¡æ¯
    lines.append("## æ¥å£åŸºæœ¬ä¿¡æ¯")
    lines.append("")
    lines.append(f"- **æ¥å£åç§°**: {endpoint['name']}")
    lines.append(f"- **è¯·æ±‚è·¯å¾„**: {endpoint['method']} {endpoint['path']}")
    lines.append(f"- **æ§åˆ¶å™¨**: {endpoint['controller']}")
    lines.append(f"- **å¤„ç†æ–¹æ³•**: {endpoint['handler']}")
    lines.append(f"- **æºæ–‡ä»¶**: {endpoint['file_path']}")
    lines.append(f"- **è¡Œå·**: {endpoint['line_number']}")
    lines.append(f"- **è§£ææ–¹æ³•**: {parse_method.upper()} ({'ASTè¯­æ³•æ ‘è§£æ' if parse_method == 'ast' else 'æ­£åˆ™è¡¨è¾¾å¼è§£æ'})")
    lines.append("")
    
    # æµ…å±‚è°ƒç”¨é“¾ï¼ˆåŸæœ‰æ•°æ®ï¼‰
    lines.append("## æµ…å±‚è°ƒç”¨é“¾")
    lines.append("")
    method_calls = call_chain.get('method_calls', [])
    if method_calls:
        lines.append("```")
        for i, call in enumerate(method_calls, 1):
            obj = call.get('object', '')
            method = call.get('method', '')
            args = call.get('arguments', 0)
            line = call.get('position', 0)
            if obj:
                lines.append(f"{i:2d}. {obj}.{method}() - {args}ä¸ªå‚æ•° (è¡Œ:{line})")
            else:
                lines.append(f"{i:2d}. {method}() - {args}ä¸ªå‚æ•° (è¡Œ:{line})")
        lines.append("```")
    else:
        lines.append("æ— æ–¹æ³•è°ƒç”¨")
    lines.append("")
    
    # æ·±åº¦è°ƒç”¨æ ‘
    lines.append("## æ·±åº¦è°ƒç”¨æ ‘")
    lines.append("")
    
    if "error" in deep_analysis:
        lines.append(f"âŒ åˆ†æå¤±è´¥: {deep_analysis['error']}")
    else:
        lines.append("```")
        lines.append(f"ğŸ“ {endpoint['handler']}() - ä¸»æ–¹æ³•")
        _build_tree_recursive_enhanced(deep_analysis.get('calls', []), lines, "  ", set(), endpoint['handler'])
        lines.append("```")
    
    lines.append("")
    
    # æ¥å£å®ç°åˆ†æ
    lines.append("## æ¥å£å®ç°åˆ†æ")
    lines.append("")
    _build_implementation_analysis(deep_analysis.get('calls', []), lines)
    
    # è°ƒç”¨é“¾è¯¦ç»†è¯´æ˜
    lines.append("## è°ƒç”¨é“¾è¯¦ç»†è¯´æ˜")
    lines.append("")
    _build_detailed_explanation_enhanced(deep_analysis.get('calls', []), lines, 1)
    
    # æ€§èƒ½åˆ†æå»ºè®®
    lines.append("## æ€§èƒ½åˆ†æå»ºè®®")
    lines.append("")
    total_calls = _count_total_calls_enhanced(deep_analysis.get('calls', []))
    max_depth = _get_max_depth_enhanced(deep_analysis.get('calls', []))
    
    if total_calls > 30:
        lines.append("âš ï¸ **é«˜å¤æ‚åº¦æ¥å£**: è°ƒç”¨é“¾éå¸¸å¤æ‚ï¼Œå¼ºçƒˆå»ºè®®é‡æ„")
    elif total_calls > 20:
        lines.append("âš¡ **ä¸­é«˜å¤æ‚åº¦**: è°ƒç”¨é“¾è¾ƒæ·±ï¼Œå»ºè®®è€ƒè™‘é‡æ„")
    elif total_calls > 10:
        lines.append("âš¡ **ä¸­ç­‰å¤æ‚åº¦**: è°ƒç”¨é“¾é€‚ä¸­ï¼Œæ³¨æ„æ€§èƒ½ç›‘æ§")
    else:
        lines.append("âœ… **ç®€å•æ¥å£**: è°ƒç”¨é“¾ç®€æ´ï¼Œæ€§èƒ½è‰¯å¥½")
    
    lines.append(f"- æ€»è°ƒç”¨æ•°: {total_calls}")
    lines.append(f"- æœ€å¤§æ·±åº¦: {max_depth}")
    lines.append(f"- æ¥å£å®ç°æ•°: {_count_interface_implementations(deep_analysis.get('calls', []))}")
    lines.append("")
    
    # ä¼˜åŒ–å»ºè®®
    lines.append("### ä¼˜åŒ–å»ºè®®")
    lines.append("")
    lines.append("1. **å‡å°‘ä¸å¿…è¦çš„æ–¹æ³•è°ƒç”¨**: åˆå¹¶ç›¸ä¼¼çš„æ“ä½œ")
    lines.append("2. **ç¼“å­˜é‡å¤è®¡ç®—**: å¯¹äºé‡å¤çš„è®¡ç®—ç»“æœè¿›è¡Œç¼“å­˜")
    lines.append("3. **å¼‚æ­¥å¤„ç†**: å¯¹äºè€—æ—¶æ“ä½œè€ƒè™‘å¼‚æ­¥å¤„ç†")
    lines.append("4. **æ‰¹é‡æ“ä½œ**: å‡å°‘æ•°æ®åº“äº¤äº’æ¬¡æ•°")
    lines.append("5. **æ¥å£ä¼˜åŒ–**: è€ƒè™‘ä½¿ç”¨å…·ä½“å®ç°ç±»è€Œéæ¥å£è°ƒç”¨")
    lines.append("")
    
    return "\n".join(lines)

def _build_tree_recursive_enhanced(calls: List[Dict], lines: List[str], indent: str, visited_methods: set = None, current_method: str = ""):
    """é€’å½’æ„å»ºè°ƒç”¨æ ‘ - å¢å¼ºç‰ˆï¼Œé¿å…é‡å¤æ˜¾ç¤º"""
    if visited_methods is None:
        visited_methods = set()
    
    for call in calls:
        method = call.get('method', 'unknown')
        obj = call.get('object', '')
        line_num = call.get('line', 0)
        args = call.get('arguments', 0)
        call_type = call.get('type', 'instance')
        
        # æ„å»ºè°ƒç”¨æ˜¾ç¤º
        if obj:
            call_display = f"{obj}.{method}()"
        else:
            call_display = f"{method}()"
        
        # åˆ›å»ºæ–¹æ³•æ ‡è¯†ç¬¦ç”¨äºé¿å…é‡å¤æ˜¾ç¤º
        method_id = f"{obj}.{method}" if obj else method
        
        # è·³è¿‡é€’å½’è°ƒç”¨è‡ªå·±çš„æƒ…å†µ
        if method == current_method and call_type == "direct":
            lines.append(f"{indent}â”œâ”€â”€ {call_display} [é€’å½’è°ƒç”¨] - {args}ä¸ªå‚æ•° (è¡Œ:{line_num})")
            continue
        
        # æ·»åŠ ç±»å‹æ ‡è¯†
        type_marker = ""
        if call_type == "static":
            type_marker = " [é™æ€]"
        elif call_type == "constructor":
            type_marker = " [æ„é€ ]"
        elif call_type == "chain":
            type_marker = " [é“¾å¼]"
        elif call_type == "enum_constant":
            type_marker = " [æšä¸¾]"
        
        lines.append(f"{indent}â”œâ”€â”€ {call_display}{type_marker} - {args}ä¸ªå‚æ•° (è¡Œ:{line_num})")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¾ç¤ºè¿‡è¿™ä¸ªæ–¹æ³•ï¼ˆé¿å…å¾ªç¯æ˜¾ç¤ºï¼‰
        full_method_id = f"{method_id}@{line_num}"
        if full_method_id in visited_methods:
            lines.append(f"{indent}  â””â”€â”€ [å·²åˆ†æè¿‡ï¼Œé¿å…é‡å¤æ˜¾ç¤º]")
            continue
        
        visited_methods.add(full_method_id)
        
        # å¤„ç†å¤šä¸ªå®ç°
        implementations = call.get('implementations', [])
        if implementations:
            # è¿‡æ»¤æ‰æ ‡å‡†åº“å’Œæšä¸¾ç±»çš„å®ç°ï¼Œé¿å…è¿‡åº¦å±•å¼€
            filtered_impls = [impl for impl in implementations 
                            if impl.get('type') not in ['standard_library', 'enum_class']]
            
            if not filtered_impls:
                # å¦‚æœåªæœ‰æ ‡å‡†åº“å®ç°ï¼Œç®€å•æ˜¾ç¤º
                std_impls = [impl for impl in implementations 
                           if impl.get('type') in ['standard_library', 'enum_class']]
                if std_impls:
                    impl = std_impls[0]
                    impl_class = impl.get('class', 'unknown')
                    impl_type = impl.get('type', 'concrete')
                    type_desc = 'æ ‡å‡†åº“' if impl_type == 'standard_library' else 'æšä¸¾ç±»'
                    lines.append(f"{indent}  â””â”€â”€ {impl_class} ({type_desc})")
                continue
            
            # åªæ˜¾ç¤ºæœ€ç›¸å…³çš„å®ç°ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªï¼‰
            impl = filtered_impls[0]
            impl_type = impl.get('type', 'concrete')
            impl_class = impl.get('class', 'unknown')
            
            type_desc = {
                'concrete': 'å…·ä½“å®ç°',
                'interface_implementation': 'æ¥å£å®ç°',
                'inheritance': 'ç»§æ‰¿å®ç°',
                'local': 'æœ¬åœ°æ–¹æ³•',
                'service_implementation': 'Serviceå®ç°',
                'service_interface': 'Serviceæ¥å£',
                'project_class': 'é¡¹ç›®ç±»',
                'similar_match': 'ç›¸ä¼¼åŒ¹é…'
            }.get(impl_type, 'æœªçŸ¥ç±»å‹')
            
            # å¯¹äºæœ¬åœ°æ–¹æ³•ï¼Œä¸æ˜¾ç¤ºå®ç°è¯¦æƒ…ï¼Œç›´æ¥å±•å¼€å­è°ƒç”¨
            if impl_type == 'local':
                sub_calls = impl.get('sub_calls', {})
                if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                    # è¿‡æ»¤æ‰ä¸å½“å‰æ–¹æ³•ç›¸åŒçš„è°ƒç”¨ï¼Œé¿å…æ— é™é€’å½’æ˜¾ç¤º
                    filtered_sub_calls = [sc for sc in sub_calls['calls'] 
                                        if sc.get('method') != method or sc.get('object') != obj]
                    if filtered_sub_calls:
                        _build_tree_recursive_enhanced(filtered_sub_calls, lines, indent + "  ", visited_methods.copy(), method)
                elif isinstance(sub_calls, dict) and 'note' in sub_calls:
                    lines.append(f"{indent}  â””â”€â”€ {sub_calls['note']}")
            else:
                lines.append(f"{indent}  â””â”€â”€ {impl_class} ({type_desc})")
                
                # é€’å½’å¤„ç†å­è°ƒç”¨
                sub_calls = impl.get('sub_calls', {})
                if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                    _build_tree_recursive_enhanced(sub_calls['calls'], lines, indent + "    ", visited_methods.copy(), method)
                elif isinstance(sub_calls, dict) and 'note' in sub_calls:
                    lines.append(f"{indent}    â””â”€â”€ {sub_calls['note']}")
        else:
            # å¤„ç†å•ä¸ªå®ç°ï¼ˆå‘åå…¼å®¹ï¼‰
            sub_calls = call.get('sub_calls', {})
            if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                # è¿‡æ»¤æ‰ä¸å½“å‰æ–¹æ³•ç›¸åŒçš„è°ƒç”¨
                filtered_sub_calls = [sc for sc in sub_calls['calls'] 
                                    if sc.get('method') != method or sc.get('object') != obj]
                if filtered_sub_calls:
                    _build_tree_recursive_enhanced(filtered_sub_calls, lines, indent + "  ", visited_methods.copy(), method)
            elif isinstance(sub_calls, dict) and 'note' in sub_calls:
                lines.append(f"{indent}  â””â”€â”€ {sub_calls['note']}")
        
        # ä»å·²è®¿é—®é›†åˆä¸­ç§»é™¤ï¼Œå…è®¸åœ¨ä¸åŒåˆ†æ”¯ä¸­é‡æ–°æ˜¾ç¤º
        visited_methods.discard(full_method_id)

def _build_implementation_analysis(calls: List[Dict], lines: List[str]):
    """æ„å»ºæ¥å£å®ç°åˆ†æ"""
    interface_calls = []
    concrete_calls = []
    
    def collect_implementations(call_list, depth=0):
        for call in call_list:
            implementations = call.get('implementations', [])
            if implementations:
                for impl in implementations:
                    if impl.get('type') == 'interface_implementation':
                        interface_calls.append({
                            'method': call.get('method', 'unknown'),
                            'object': call.get('object', ''),
                            'implementation': impl,
                            'depth': depth
                        })
                    elif impl.get('type') == 'concrete':
                        concrete_calls.append({
                            'method': call.get('method', 'unknown'),
                            'object': call.get('object', ''),
                            'implementation': impl,
                            'depth': depth
                        })
                    
                    # é€’å½’æ”¶é›†
                    sub_calls = impl.get('sub_calls', {})
                    if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                        collect_implementations(sub_calls['calls'], depth + 1)
    
    collect_implementations(calls)
    
    if interface_calls:
        lines.append("### æ¥å£è°ƒç”¨")
        lines.append("")
        for call in interface_calls[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            method = call['method']
            obj = call['object']
            impl_class = call['implementation'].get('class', 'unknown')
            lines.append(f"- **{obj}.{method}()** â†’ {impl_class} (æ·±åº¦: {call['depth']})")
        
        if len(interface_calls) > 5:
            lines.append(f"- ... è¿˜æœ‰ {len(interface_calls) - 5} ä¸ªæ¥å£è°ƒç”¨")
        lines.append("")
    
    if concrete_calls:
        lines.append("### å…·ä½“ç±»è°ƒç”¨")
        lines.append("")
        for call in concrete_calls[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            method = call['method']
            obj = call['object']
            impl_class = call['implementation'].get('class', 'unknown')
            lines.append(f"- **{obj}.{method}()** â†’ {impl_class} (æ·±åº¦: {call['depth']})")
        
        if len(concrete_calls) > 5:
            lines.append(f"- ... è¿˜æœ‰ {len(concrete_calls) - 5} ä¸ªå…·ä½“ç±»è°ƒç”¨")
        lines.append("")

def _build_detailed_explanation_enhanced(calls: List[Dict], lines: List[str], level: int):
    """æ„å»ºè¯¦ç»†è¯´æ˜ - å¢å¼ºç‰ˆ"""
    for i, call in enumerate(calls, 1):
        method = call.get('method', 'unknown')
        obj = call.get('object', '')
        call_type = call.get('type', 'instance')
        
        lines.append(f"### {level}.{i} {obj}.{method}() è°ƒç”¨" if obj else f"### {level}.{i} {method}() è°ƒç”¨")
        lines.append("")
        
        lines.append(f"- **è°ƒç”¨ç±»å‹**: {call_type}")
        lines.append(f"- **å‚æ•°æ•°é‡**: {call.get('arguments', 0)}")
        lines.append(f"- **è°ƒç”¨è¡Œå·**: {call.get('line', 0)}")
        
        # å®ç°ä¿¡æ¯
        implementations = call.get('implementations', [])
        if implementations:
            lines.append(f"- **å®ç°æ•°é‡**: {len(implementations)}")
            lines.append("")
            lines.append("**å®ç°è¯¦æƒ…**:")
            
            for j, impl in enumerate(implementations, 1):
                impl_type = impl.get('type', 'concrete')
                impl_class = impl.get('class', 'unknown')
                impl_file = impl.get('file', '')
                
                lines.append(f"  {j}. **{impl_class}** ({impl_type})")
                if impl_file:
                    lines.append(f"     - æ–‡ä»¶: {impl_file}")
                
                # å­è°ƒç”¨ç»Ÿè®¡
                sub_calls = impl.get('sub_calls', {})
                if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                    sub_count = len(sub_calls['calls'])
                    lines.append(f"     - å­è°ƒç”¨: {sub_count} ä¸ª")
        else:
            # å‘åå…¼å®¹
            impl = call.get('implementation', '')
            if impl:
                lines.append(f"- **å®ç°ä½ç½®**: {impl}")
        
        lines.append("")

def _count_total_calls_enhanced(calls: List[Dict]) -> int:
    """è®¡ç®—æ€»è°ƒç”¨æ•° - å¢å¼ºç‰ˆ"""
    total = len(calls)
    for call in calls:
        implementations = call.get('implementations', [])
        if implementations:
            for impl in implementations:
                sub_calls = impl.get('sub_calls', {})
                if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                    total += _count_total_calls_enhanced(sub_calls['calls'])
        else:
            # å‘åå…¼å®¹
            sub_calls = call.get('sub_calls', {})
            if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                total += _count_total_calls_enhanced(sub_calls['calls'])
    return total

def _get_max_depth_enhanced(calls: List[Dict], current_depth: int = 1) -> int:
    """è·å–æœ€å¤§æ·±åº¦ - å¢å¼ºç‰ˆ"""
    max_depth = current_depth
    for call in calls:
        implementations = call.get('implementations', [])
        if implementations:
            for impl in implementations:
                sub_calls = impl.get('sub_calls', {})
                if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                    depth = _get_max_depth_enhanced(sub_calls['calls'], current_depth + 1)
                    max_depth = max(max_depth, depth)
        else:
            # å‘åå…¼å®¹
            sub_calls = call.get('sub_calls', {})
            if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                depth = _get_max_depth_enhanced(sub_calls['calls'], current_depth + 1)
                max_depth = max(max_depth, depth)
    return max_depth

def _count_interface_implementations(calls: List[Dict]) -> int:
    """ç»Ÿè®¡æ¥å£å®ç°æ•°é‡"""
    count = 0
    
    def count_recursive(call_list):
        nonlocal count
        for call in call_list:
            implementations = call.get('implementations', [])
            for impl in implementations:
                if impl.get('type') == 'interface_implementation':
                    count += 1
                
                sub_calls = impl.get('sub_calls', {})
                if isinstance(sub_calls, dict) and 'calls' in sub_calls:
                    count_recursive(sub_calls['calls'])
    
    count_recursive(calls)
    return count

def show_endpoint_details(endpoint_path: str, output_dir: str = "./migration_output"):
    """æ˜¾ç¤ºç‰¹å®šæ¥å£çš„ä»£ç å’Œè°ƒç”¨é“¾"""
    analysis_file = f"{output_dir}/endpoint_analysis.json"
    
    if not os.path.exists(analysis_file):
        print(f"âŒ åˆ†ææ–‡ä»¶ä¸å­˜åœ¨: {analysis_file}")
        print("è¯·å…ˆè¿è¡Œå•é¡¹ç›®åˆ†æç”Ÿæˆåˆ†ææ•°æ®ï¼š")
        print("python main.py --single /path/to/project")
        return
    
    # åŠ è½½åˆ†ææ•°æ®
    try:
        with open(analysis_file, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–åˆ†ææ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ¥å£
    matching_endpoints = []
    for endpoint_data in analysis_data:
        endpoint = endpoint_data['endpoint']
        if endpoint_path in endpoint['path'] or endpoint_path == endpoint['path']:
            matching_endpoints.append(endpoint_data)
    
    if not matching_endpoints:
        print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ¥å£: {endpoint_path}")
        print("\nå¯ç”¨çš„æ¥å£è·¯å¾„:")
        for endpoint_data in analysis_data[:10]:  # æ˜¾ç¤ºå‰10ä¸ªä½œä¸ºç¤ºä¾‹
            endpoint = endpoint_data['endpoint']
            print(f"  - {endpoint['method']} {endpoint['path']}")
        if len(analysis_data) > 10:
            print(f"  ... è¿˜æœ‰ {len(analysis_data) - 10} ä¸ªæ¥å£")
        return
    
    # æ˜¾ç¤ºåŒ¹é…çš„æ¥å£
    if len(matching_endpoints) > 1:
        print(f"ğŸ” æ‰¾åˆ° {len(matching_endpoints)} ä¸ªåŒ¹é…çš„æ¥å£:")
        for i, endpoint_data in enumerate(matching_endpoints, 1):
            endpoint = endpoint_data['endpoint']
            print(f"{i}. {endpoint['method']} {endpoint['path']} - {endpoint['name']}")
        
        try:
            choice = int(input("\nè¯·é€‰æ‹©è¦æŸ¥çœ‹çš„æ¥å£ (è¾“å…¥åºå·): ")) - 1
            if 0 <= choice < len(matching_endpoints):
                selected_endpoint = matching_endpoints[choice]
            else:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©")
                return
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            return
    else:
        selected_endpoint = matching_endpoints[0]
    
    # æ˜¾ç¤ºæ¥å£è¯¦ç»†ä¿¡æ¯
    _display_endpoint_details(selected_endpoint)

def _display_endpoint_details(endpoint_data: Dict):
    """æ˜¾ç¤ºæ¥å£çš„è¯¦ç»†ä¿¡æ¯"""
    endpoint = endpoint_data['endpoint']
    call_chain = endpoint_data['call_chain']
    sql_mappings = endpoint_data.get('sql_mappings', [])
    complexity_score = endpoint_data['complexity_score']
    
    print(f"\n{'='*80}")
    print(f"ğŸ” æ¥å£è¯¦ç»†ä¿¡æ¯")
    print(f"{'='*80}")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ¥å£åç§°: {endpoint['name']}")
    print(f"  è¯·æ±‚è·¯å¾„: {endpoint['method']} {endpoint['path']}")
    print(f"  æ§åˆ¶å™¨: {endpoint['controller']}")
    print(f"  å¤„ç†æ–¹æ³•: {endpoint['handler']}")
    print(f"  æºæ–‡ä»¶: {endpoint['file_path']}")
    print(f"  è¡Œå·: {endpoint['line_number']}")
    print(f"  å¤æ‚åº¦: {complexity_score}")
    print(f"  æ¡†æ¶: {endpoint['framework']}")
    print()
    
    # è°ƒç”¨é“¾åˆ†æ
    print(f"ğŸ”— è°ƒç”¨é“¾åˆ†æ:")
    method_calls = call_chain.get('method_calls', [])
    if method_calls:
        print(f"  æ–¹æ³•è°ƒç”¨ ({len(method_calls)}ä¸ª):")
        for i, call in enumerate(method_calls, 1):
            obj = call.get('object', 'unknown')
            method = call.get('method', 'unknown')
            args = call.get('arguments', 0)
            position = call.get('position', 0)
            print(f"    {i:2d}. {obj}.{method}() - {args}ä¸ªå‚æ•° (è¡Œ:{position})")
    else:
        print("  æ— å¤æ‚æ–¹æ³•è°ƒç”¨")
    print()
    
    # ç›¸å…³æ–‡ä»¶
    files = call_chain.get('files', [])
    if files:
        print(f"ğŸ“ ç›¸å…³æ–‡ä»¶ ({len(files)}ä¸ª):")
        # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
        service_files = [f for f in files if 'service' in f.get('path', '').lower()]
        dto_files = [f for f in files if 'dto' in f.get('path', '').lower()]
        vo_files = [f for f in files if 'vo' in f.get('path', '').lower()]
        mapper_files = [f for f in files if 'mapper' in f.get('path', '').lower()]
        
        if service_files:
            print("  Serviceå±‚:")
            for file in service_files[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                file_name = Path(file['path']).name
                print(f"    - {file_name}")
        
        if dto_files:
            print("  DTOå¯¹è±¡:")
            for file in dto_files[:3]:
                file_name = Path(file['path']).name
                print(f"    - {file_name}")
        
        if vo_files:
            print("  VOå¯¹è±¡:")
            for file in vo_files[:3]:
                file_name = Path(file['path']).name
                print(f"    - {file_name}")
        
        if mapper_files:
            print("  Mapperå±‚:")
            for file in mapper_files[:3]:
                file_name = Path(file['path']).name
                print(f"    - {file_name}")
    print()
    
    # SQLæ˜ å°„ä¿¡æ¯
    if sql_mappings:
        print(f"ğŸ—„ï¸  SQLæ˜ å°„ä¿¡æ¯:")
        for mapping in sql_mappings[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
            file_path = mapping.get('file_path', '')
            file_name = Path(file_path).name if file_path else 'unknown'
            methods = mapping.get('methods', [])
            print(f"  {file_name}:")
            for method in methods[:2]:  # æ¯ä¸ªæ–‡ä»¶æœ€å¤šæ˜¾ç¤º2ä¸ªæ–¹æ³•
                method_id = method.get('id', 'unknown')
                sql_type = method.get('type', 'unknown')
                sql = method.get('sql', '')[:60] + '...' if len(method.get('sql', '')) > 60 else method.get('sql', '')
                print(f"    - {method_id} ({sql_type}): {sql}")
    print()
    
    # å°è¯•è¯»å–å¹¶æ˜¾ç¤ºæºä»£ç 
    _display_source_code(endpoint)

def _display_source_code(endpoint: Dict):
    """æ˜¾ç¤ºæºä»£ç """
    file_path = endpoint['file_path']
    line_number = endpoint['line_number']
    handler_name = endpoint['handler']
    
    print(f"ğŸ“ æºä»£ç ç‰‡æ®µ:")
    
    try:
        # å°è¯•è¯»å–æºæ–‡ä»¶
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æŸ¥æ‰¾æ–¹æ³•å®šä¹‰
            method_start = -1
            method_end = -1
            brace_count = 0
            in_method = False
            
            # ä»æŒ‡å®šè¡Œå·å¼€å§‹å‘å‰æŸ¥æ‰¾æ–¹æ³•å®šä¹‰
            for i in range(max(0, line_number - 10), min(len(lines), line_number + 50)):
                line = lines[i].strip()
                
                # æŸ¥æ‰¾æ–¹æ³•å®šä¹‰
                if handler_name in lines[i] and ('public' in lines[i] or 'private' in lines[i] or 'protected' in lines[i]):
                    method_start = i
                    in_method = True
                    brace_count = 0
                
                if in_method:
                    # è®¡ç®—å¤§æ‹¬å·
                    brace_count += line.count('{') - line.count('}')
                    
                    # å¦‚æœå¤§æ‹¬å·å¹³è¡¡ï¼Œè¯´æ˜æ–¹æ³•ç»“æŸ
                    if brace_count == 0 and method_start != -1 and i > method_start:
                        method_end = i
                        break
            
            # æ˜¾ç¤ºæ–¹æ³•ä»£ç 
            if method_start != -1:
                print(f"  æ–‡ä»¶: {Path(file_path).name}")
                print(f"  æ–¹æ³•: {handler_name} (è¡Œ {method_start + 1}-{method_end + 1 if method_end != -1 else '?'})")
                print("  " + "-" * 60)
                
                end_line = method_end if method_end != -1 else min(method_start + 20, len(lines))
                for i in range(method_start, end_line + 1):
                    if i < len(lines):
                        line_num = i + 1
                        code_line = lines[i].rstrip()
                        # é«˜äº®å½“å‰è¡Œ
                        marker = ">>>" if line_num == line_number else "   "
                        print(f"  {marker} {line_num:3d}: {code_line}")
            else:
                print(f"  âŒ æ— æ³•æ‰¾åˆ°æ–¹æ³• {handler_name} çš„å®šä¹‰")
        else:
            print(f"  âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    except Exception as e:
        print(f"  âŒ è¯»å–æºæ–‡ä»¶å¤±è´¥: {e}")
    
    print(f"\n{'='*80}")
    print("âœ… æ¥å£åˆ†æå®Œæˆ")
    print(f"{'='*80}\n")

@dataclass
class Config:
    """é…ç½®ç±»"""
    old_project_path: Optional[str] = None
    new_project_path: Optional[str] = None
    single_project_path: Optional[str] = None  # æ–°å¢ï¼šå•é¡¹ç›®æ¨¡å¼
    output_dir: str = "./migration_output"
    ai_model: str = "gpt-3.5-turbo"
    api_key: Optional[str] = None
    context_window: int = 4000
    verbose: bool = False
    analyze_only: bool = False  # ä»…åˆ†ææ¨¡å¼
    single_mode: bool = False   # æ–°å¢ï¼šå•é¡¹ç›®æ¨¡å¼æ ‡å¿—

class MigrationTool:
    """è¿ç§»å·¥å…·ä¸»ç±»"""
    
    def __init__(self, config: Config):
        self.config = config
        self.endpoint_extractor = EndpointExtractor()
        self.equivalence_matcher = EquivalenceMatcher()
        self.call_chain_analyzer = CallChainAnalyzer()
        self.sql_mapper_analyzer = SQLMapperAnalyzer()
        
        # ä»…åœ¨é…ç½®äº†APIå¯†é’¥æ—¶åˆå§‹åŒ–AIç”Ÿæˆå™¨
        if not config.analyze_only and (config.api_key or os.getenv("OPENAI_API_KEY")):
            self.ai_generator = AIGenerator(
                model=config.ai_model,
                api_key=config.api_key or os.getenv("OPENAI_API_KEY")
            )
        else:
            self.ai_generator = None
            if not config.single_mode:
                print("âš ï¸  AIåŠŸèƒ½å·²ç¦ç”¨ï¼Œä»…æ‰§è¡Œåˆ†æ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.output_dir, exist_ok=True)
        
    def run(self):
        """è¿è¡Œå®Œæ•´çš„è¿ç§»æµç¨‹"""
        if self.config.single_mode:
            self.run_single_project_analysis()
        else:
            self.run_migration_analysis()
    
    def run_single_project_analysis(self):
        """è¿è¡Œå•é¡¹ç›®åˆ†æ"""
        print("ğŸš€ å¼€å§‹åˆ†æé¡¹ç›®æ¥å£...")
        
        # æå–æ¥å£
        print("ğŸ“‹ æå–é¡¹ç›®æ¥å£...")
        endpoints = self.endpoint_extractor.extract_from_project(
            self.config.single_project_path
        )
        
        print(f"âœ… æå–å®Œæˆ: å…±æ‰¾åˆ° {len(endpoints)} ä¸ªæ¥å£")
        
        # æ˜¾ç¤ºè§£æçš„æ¥å£ç»“æ„
        if self.config.verbose:
            self.display_endpoints("é¡¹ç›®æ¥å£ç»“æ„", endpoints)
        
        # åˆ†ææ¯ä¸ªæ¥å£çš„è°ƒç”¨é“¾
        print("ğŸ” åˆ†ææ¥å£è°ƒç”¨é“¾å’Œä¾èµ–...")
        endpoint_analysis = []
        
        total_endpoints = len(endpoints)
        for i, (name, endpoint) in enumerate(endpoints.items(), 1):
            print(f"  ğŸ“Š åˆ†æè¿›åº¦: {i}/{total_endpoints} ({i/total_endpoints*100:.1f}%) - {endpoint.name}")
            
            # åˆ†æè°ƒç”¨é“¾
            call_chain = self.call_chain_analyzer.analyze_call_chain(
                endpoint, self.config.single_project_path
            )
            
            # åˆ†æSQLæ˜ å°„
            sql_mappings = self.sql_mapper_analyzer.find_related_mappers(
                call_chain, self.config.single_project_path
            )
            
            analysis = {
                "endpoint": endpoint,
                "call_chain": call_chain,
                "sql_mappings": sql_mappings,
                "complexity_score": self._calculate_complexity_score(call_chain, sql_mappings)
            }
            
            endpoint_analysis.append(analysis)
        
        print("âœ… æ¥å£åˆ†æå®Œæˆ")
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        if self.config.verbose:
            self.display_single_project_analysis(endpoint_analysis)
        
        # ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        self.save_single_project_results(endpoints, endpoint_analysis)
        
        print(f"ğŸ‰ å•é¡¹ç›®åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {self.config.output_dir}")
        print(f"ğŸ“‹ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æ¥å£è¯¦æƒ…:")
        print(f"   python main.py --show-endpoint <æ¥å£è·¯å¾„> --output {self.config.output_dir}")
        print(f"ğŸ“‹ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆè°ƒç”¨é“¾æ ‘:")
        print(f"   python main.py --call-tree <æ¥å£è·¯å¾„> --output {self.config.output_dir}")
    
    def run_migration_analysis(self):
        """è¿è¡Œè¿ç§»åˆ†æï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        print("ğŸš€ å¼€å§‹åˆ†ææ–°æ—§é¡¹ç›®æ¥å£...")
        
        # 1. æå–æ¥å£
        print("ğŸ“‹ æ­¥éª¤1: æå–æ—§é¡¹ç›®æ¥å£...")
        old_endpoints = self.endpoint_extractor.extract_from_project(
            self.config.old_project_path
        )
        
        print(f"ğŸ“‹ æ­¥éª¤2: æå–æ–°é¡¹ç›®æ¥å£...")
        new_endpoints = self.endpoint_extractor.extract_from_project(
            self.config.new_project_path
        )
        
        print(f"âœ… æå–å®Œæˆ: æ—§æ¥å£ {len(old_endpoints)} ä¸ª, æ–°æ¥å£ {len(new_endpoints)} ä¸ª")
        
        # æ˜¾ç¤ºè§£æçš„æ¥å£ç»“æ„
        if self.config.verbose:
            self.display_endpoints("æ—§é¡¹ç›®æ¥å£ç»“æ„", old_endpoints)
            self.display_endpoints("æ–°é¡¹ç›®æ¥å£ç»“æ„", new_endpoints)
        
        # 2. åŒ¹é…ç­‰ä»·æ¥å£
        print("ğŸ”„ æ­¥éª¤3: åŒ¹é…ç­‰ä»·æ¥å£...")
        matched_pairs = self.equivalence_matcher.match_endpoints(
            old_endpoints, new_endpoints
        )
        
        print(f"âœ… åŒ¹é…å®Œæˆ: æ‰¾åˆ° {len(matched_pairs)} å¯¹ç­‰ä»·æ¥å£")
        
        # æ˜¾ç¤ºåŒ¹é…çš„æ¥å£å¯¹
        if self.config.verbose and matched_pairs:
            self.display_matched_pairs(matched_pairs)
        
        # 3. åˆ†æè°ƒç”¨é“¾å’ŒSQLæ˜ å°„
        print("ğŸ” æ­¥éª¤4: åˆ†æè°ƒç”¨é“¾å’Œä¾èµ–...")
        migration_plan = self.analyze_migration_plan(matched_pairs)
        
        # æ˜¾ç¤ºè°ƒç”¨é“¾ä¿¡æ¯
        if self.config.verbose and migration_plan:
            self.display_call_chains(migration_plan)
        
        # 4. ä»…åœ¨å¯ç”¨AIåŠŸèƒ½æ—¶ç”Ÿæˆè¿ç§»ä»£ç 
        if self.ai_generator:
            print("ğŸ¤– æ­¥éª¤5: ç”Ÿæˆè¿ç§»ä»£ç ...")
            generated_code = self.generate_migration_code(migration_plan)
        else:
            generated_code = {}
            print("â­ï¸  è·³è¿‡ä»£ç ç”Ÿæˆæ­¥éª¤ï¼ˆæœªå¯ç”¨AIåŠŸèƒ½ï¼‰")
        
        # 5. ä¿å­˜ç»“æœ
        print("ğŸ’¾ æ­¥éª¤6: ä¿å­˜ç»“æœ...")
        self.save_results(old_endpoints, new_endpoints, matched_pairs, generated_code)
        
        print(f"ğŸ‰ {'åˆ†æ' if self.config.analyze_only else 'è¿ç§»'}å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {self.config.output_dir}")
    
    def display_endpoints(self, title: str, endpoints: Dict):
        """æ˜¾ç¤ºè§£æçš„æ¥å£ç»“æ„"""
        print(f"\n=== {title} ===")
        for i, (name, endpoint) in enumerate(endpoints.items(), 1):
            print(f"{i}. {name}:")
            print(f"  è·¯å¾„: {endpoint.path}")
            print(f"  æ–¹æ³•: {endpoint.method}")
            print(f"  æ§åˆ¶å™¨: {endpoint.controller}")
            print(f"  å¤„ç†å™¨: {endpoint.handler}")
            print(f"  æ–‡ä»¶: {endpoint.file_path}")
            print(f"  è¡Œå·: {endpoint.line_number}")
            print("-" * 40)
    
    def display_matched_pairs(self, matched_pairs: List):
        """æ˜¾ç¤ºåŒ¹é…çš„æ¥å£å¯¹"""
        print("\n=== ç­‰ä»·æ¥å£åŒ¹é…ç»“æœ ===")
        for i, (old_ep, new_ep) in enumerate(matched_pairs, 1):
            print(f"{i}. åŒ¹é…å¯¹:")
            print(f"  æ—§æ¥å£: {old_ep.name} ({old_ep.method} {old_ep.path})")
            print(f"  æ–°æ¥å£: {new_ep.name} ({new_ep.method} {new_ep.path})")
            print(f"  ç›¸ä¼¼åº¦: {getattr(old_ep, 'match_score', {}).get('total_score', 0):.2f}")
            print("-" * 60)
    
    def display_single_project_analysis(self, endpoint_analysis: List[Dict]):
        """æ˜¾ç¤ºå•é¡¹ç›®åˆ†æç»“æœ"""
        print("\n=== å•é¡¹ç›®æ¥å£åˆ†æç»“æœ ===")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_endpoints = len(endpoint_analysis)
        complex_endpoints = sum(1 for analysis in endpoint_analysis if analysis["complexity_score"] > 5)
        
        print(f"æ€»æ¥å£æ•°: {total_endpoints}")
        print(f"å¤æ‚æ¥å£æ•°: {complex_endpoints}")
        print(f"ç®€å•æ¥å£æ•°: {total_endpoints - complex_endpoints}")
        
        # æŒ‰å¤æ‚åº¦æ’åºæ˜¾ç¤º
        sorted_analysis = sorted(endpoint_analysis, key=lambda x: x["complexity_score"], reverse=True)
        
        for i, analysis in enumerate(sorted_analysis, 1):
            endpoint = analysis["endpoint"]
            call_chain = analysis["call_chain"]
            complexity = analysis["complexity_score"]
            
            print(f"\n{i}. æ¥å£: {endpoint.name}")
            print(f"   è·¯å¾„: {endpoint.method} {endpoint.path}")
            print(f"   æ–‡ä»¶: {endpoint.file_path}:{endpoint.line_number}")
            print(f"   å¤æ‚åº¦: {complexity}")
            
            if call_chain.get("method_calls"):
                print(f"   æ–¹æ³•è°ƒç”¨: {len(call_chain['method_calls'])} ä¸ª")
                if self.config.verbose:
                    for j, call in enumerate(call_chain["method_calls"][:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        print(f"     {j}. {call.get('object', '')}.{call.get('method', '')}()")
            
            if call_chain.get("sql_statements"):
                print(f"   SQLè¯­å¥: {len(call_chain['sql_statements'])} ä¸ª")
            
            if analysis.get("sql_mappings"):
                print(f"   SQLæ˜ å°„: {len(analysis['sql_mappings'])} ä¸ª")
            
            print("-" * 60)
    
    def display_call_chains(self, migration_plan: List[Dict]):
        """æ˜¾ç¤ºæ¥å£è°ƒç”¨é“¾ä¿¡æ¯"""
        print("\n=== æ¥å£è°ƒç”¨é“¾åˆ†æ ===")
        for i, plan in enumerate(migration_plan, 1):
            old_ep = plan["old_endpoint"]
            print(f"{i}. æ¥å£: {old_ep.name} ({old_ep.method} {old_ep.path})")
            
            call_chain = plan["call_chain"]
            if call_chain.get("method_calls"):
                print("  æ–¹æ³•è°ƒç”¨é“¾:")
                for j, call in enumerate(call_chain["method_calls"], 1):
                    print(f"    {j}. {call.get('object', '')}.{call.get('method', '')}()")
            
            if call_chain.get("service_calls"):
                print("  æœåŠ¡è°ƒç”¨:")
                for j, service in enumerate(call_chain["service_calls"], 1):
                    print(f"    {j}. {service}")
            
            if call_chain.get("dao_calls"):
                print("  DAOè°ƒç”¨:")
                for j, dao in enumerate(call_chain["dao_calls"], 1):
                    print(f"    {j}. {dao}")
            
            if call_chain.get("sql_statements"):
                print("  SQLè¯­å¥:")
                for j, sql in enumerate(call_chain["sql_statements"], 1):
                    print(f"    {j}. {sql[:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            
            print("-" * 60)
    
    def _calculate_complexity_score(self, call_chain: Dict, sql_mappings: List) -> int:
        """è®¡ç®—æ¥å£å¤æ‚åº¦å¾—åˆ†"""
        score = 0
        
        # æ–¹æ³•è°ƒç”¨æ•°é‡
        method_calls = len(call_chain.get("method_calls", []))
        score += method_calls * 1
        
        # SQLè¯­å¥æ•°é‡
        sql_statements = len(call_chain.get("sql_statements", []))
        score += sql_statements * 2
        
        # SQLæ˜ å°„æ–‡ä»¶æ•°é‡
        score += len(sql_mappings) * 3
        
        # ç›¸å…³æ–‡ä»¶æ•°é‡
        related_files = len(call_chain.get("files", []))
        score += related_files * 1
        
        return score
    
    def analyze_migration_plan(self, matched_pairs: List) -> List[Dict]:
        """åˆ†æè¿ç§»è®¡åˆ’"""
        migration_plan = []
        
        total_pairs = len(matched_pairs)
        print(f"ğŸ” å¼€å§‹åˆ†æ {total_pairs} å¯¹åŒ¹é…æ¥å£çš„è¿ç§»è®¡åˆ’...")
        
        for i, (old_endpoint, new_endpoint) in enumerate(matched_pairs, 1):
            print(f"  ğŸ“Š åˆ†æè¿›åº¦: {i}/{total_pairs} ({i/total_pairs*100:.1f}%) - {old_endpoint.name}")
            
            # åˆ†æè°ƒç”¨é“¾
            call_chain = self.call_chain_analyzer.analyze_call_chain(
                old_endpoint, self.config.old_project_path
            )
            
            # åˆ†æSQLæ˜ å°„
            sql_mappings = self.sql_mapper_analyzer.find_related_mappers(
                call_chain, self.config.old_project_path
            )
            
            # æ”¶é›†éœ€è¦è¿ç§»çš„ä»£ç ä¸Šä¸‹æ–‡
            migration_context = self.collect_migration_context(
                old_endpoint, call_chain, sql_mappings
            )
            
            migration_plan.append({
                "old_endpoint": old_endpoint,
                "new_endpoint": new_endpoint,
                "call_chain": call_chain,
                "sql_mappings": sql_mappings,
                "migration_context": migration_context,
                "estimated_tokens": len(str(migration_context)) // 4  # ç²—ç•¥ä¼°ç®—
            })
        
        print("âœ… è¿ç§»è®¡åˆ’åˆ†æå®Œæˆ")
        return migration_plan
    
    def collect_migration_context(self, old_endpoint, call_chain, sql_mappings):
        """æ”¶é›†è¿ç§»éœ€è¦çš„ä»£ç ä¸Šä¸‹æ–‡"""
        context = {
            "old_endpoint": old_endpoint.__dict__ if hasattr(old_endpoint, '__dict__') else old_endpoint,
            "call_chain": call_chain,
            "sql_mappings": sql_mappings,
            "related_files": set()
        }
        
        # æ”¶é›†æ‰€æœ‰ç›¸å…³æ–‡ä»¶å†…å®¹
        project_root = Path(self.config.old_project_path)
        
        for file_info in call_chain.get("files", []):
            file_path = project_root / file_info["path"]
            if file_path.exists():
                try:
                    content = file_path.read_text(encoding='utf-8')
                    context["related_files"].add({
                        "path": str(file_path.relative_to(project_root)),
                        "content": content[:5000]  # é™åˆ¶å¤§å°
                    })
                except:
                    continue
        
        return context
    
    def generate_migration_code(self, migration_plan: List[Dict]) -> Dict:
        """ç”Ÿæˆè¿ç§»ä»£ç """
        generated_code = {}
        
        total_plans = len(migration_plan)
        print(f"ğŸ¤– å¼€å§‹ç”Ÿæˆ {total_plans} ä¸ªæ¥å£çš„è¿ç§»ä»£ç ...")
        
        for i, plan in enumerate(migration_plan, 1):
            endpoint_name = plan["old_endpoint"].get("name", f"endpoint_{i}")
            print(f"  ğŸ“Š ç”Ÿæˆè¿›åº¦: {i}/{total_plans} ({i/total_plans*100:.1f}%) - {endpoint_name}")
            
            if plan["estimated_tokens"] > self.config.context_window:
                print(f"    âš ï¸  è­¦å‘Š: æ¥å£ä¸Šä¸‹æ–‡è¿‡å¤§ ({plan['estimated_tokens']} tokens)ï¼Œè·³è¿‡ç”Ÿæˆ")
                continue
                
            try:
                generated = self.ai_generator.generate_migration_code(plan)
                generated_code[endpoint_name] = generated
                print(f"    âœ… ç”ŸæˆæˆåŠŸ")
            except Exception as e:
                print(f"    âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        print("âœ… è¿ç§»ä»£ç ç”Ÿæˆå®Œæˆ")        
        return generated_code
    
    def save_single_project_results(self, endpoints: Dict, endpoint_analysis: List[Dict]):
        """ä¿å­˜å•é¡¹ç›®åˆ†æç»“æœ"""
        # ä¿å­˜æ¥å£ä¿¡æ¯
        with open(f"{self.config.output_dir}/endpoints.json", "w", encoding='utf-8') as f:
            json.dump([e.__dict__ for e in endpoints.values()], f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_data = []
        for analysis in endpoint_analysis:
            endpoint_dict = analysis["endpoint"].__dict__
            analysis_dict = {
                "endpoint": endpoint_dict,
                "call_chain": analysis["call_chain"],
                "sql_mappings": analysis["sql_mappings"],
                "complexity_score": analysis["complexity_score"]
            }
            analysis_data.append(analysis_dict)
        
        with open(f"{self.config.output_dir}/endpoint_analysis.json", "w", encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self._generate_analysis_report(endpoints, endpoint_analysis)
    
    def _generate_analysis_report(self, endpoints: Dict, endpoint_analysis: List[Dict]):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        report_lines.append("# é¡¹ç›®æ¥å£åˆ†ææŠ¥å‘Š\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_endpoints = len(endpoint_analysis)
        complex_endpoints = sum(1 for analysis in endpoint_analysis if analysis["complexity_score"] > 5)
        frameworks = set(ep.framework for ep in endpoints.values())
        
        report_lines.append("## ç»Ÿè®¡æ¦‚è§ˆ\n")
        report_lines.append(f"- æ€»æ¥å£æ•°: {total_endpoints}")
        report_lines.append(f"- å¤æ‚æ¥å£æ•°: {complex_endpoints}")
        report_lines.append(f"- ç®€å•æ¥å£æ•°: {total_endpoints - complex_endpoints}")
        report_lines.append(f"- ä½¿ç”¨æ¡†æ¶: {', '.join(frameworks)}")
        report_lines.append("")
        
        # æ¥å£åˆ—è¡¨
        report_lines.append("## æ¥å£è¯¦æƒ…\n")
        sorted_analysis = sorted(endpoint_analysis, key=lambda x: x["complexity_score"], reverse=True)
        
        for i, analysis in enumerate(sorted_analysis, 1):
            endpoint = analysis["endpoint"]
            complexity = analysis["complexity_score"]
            
            report_lines.append(f"### {i}. {endpoint.name}")
            report_lines.append(f"- **è·¯å¾„**: {endpoint.method} {endpoint.path}")
            report_lines.append(f"- **æ–‡ä»¶**: {endpoint.file_path}:{endpoint.line_number}")
            report_lines.append(f"- **å¤æ‚åº¦**: {complexity}")
            report_lines.append(f"- **æ¡†æ¶**: {endpoint.framework}")
            
            call_chain = analysis["call_chain"]
            if call_chain.get("method_calls"):
                report_lines.append(f"- **æ–¹æ³•è°ƒç”¨**: {len(call_chain['method_calls'])} ä¸ª")
            if call_chain.get("sql_statements"):
                report_lines.append(f"- **SQLè¯­å¥**: {len(call_chain['sql_statements'])} ä¸ª")
            if analysis.get("sql_mappings"):
                report_lines.append(f"- **SQLæ˜ å°„**: {len(analysis['sql_mappings'])} ä¸ª")
            
            report_lines.append("")
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f"{self.config.output_dir}/analysis_report.md", "w", encoding='utf-8') as f:
            f.write("\n".join(report_lines))
    
    def save_results(self, *args):
        """ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶"""
        # ä¿å­˜æ—§æ¥å£
        with open(f"{self.config.output_dir}/old_endpoints.json", "w", encoding='utf-8') as f:
            json.dump([e.__dict__ for e in args[0].values()], f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ–°æ¥å£
        with open(f"{self.config.output_dir}/new_endpoints.json", "w", encoding='utf-8') as f:
            json.dump([e.__dict__ for e in args[1].values()], f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜åŒ¹é…ç»“æœ
        matched_data = []
        for old, new in args[2]:
            matched_data.append({
                "old": old.__dict__,
                "new": new.__dict__
            })
        with open(f"{self.config.output_dir}/matched_pairs.json", "w", encoding='utf-8') as f:
            json.dump(matched_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç”Ÿæˆçš„ä»£ç 
        with open(f"{self.config.output_dir}/generated_code.json", "w", encoding='utf-8') as f:
            json.dump(args[3], f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Javaé¡¹ç›®æ¥å£åˆ†æå·¥å…·')
    
    # åˆ›å»ºäº’æ–¥ç»„ï¼šè¦ä¹ˆæ˜¯è¿ç§»æ¨¡å¼ï¼Œè¦ä¹ˆæ˜¯å•é¡¹ç›®æ¨¡å¼ï¼Œè¦ä¹ˆæ˜¯æ¥å£æŸ¥çœ‹æ¨¡å¼ï¼Œè¦ä¹ˆæ˜¯è°ƒç”¨æ ‘ç”Ÿæˆæ¨¡å¼
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--migrate', action='store_true', help='è¿ç§»æ¨¡å¼ï¼šåˆ†ææ–°æ—§ä¸¤ä¸ªé¡¹ç›®')
    mode_group.add_argument('--single', metavar='PROJECT_PATH', help='å•é¡¹ç›®æ¨¡å¼ï¼šåªåˆ†æä¸€ä¸ªé¡¹ç›®')
    mode_group.add_argument('--show-endpoint', metavar='ENDPOINT_PATH', help='æ˜¾ç¤ºç‰¹å®šæ¥å£çš„ä»£ç å’Œè°ƒç”¨é“¾ï¼Œå¦‚ï¼š/admin/category/page')
    mode_group.add_argument('--call-tree', metavar='ENDPOINT_PATH', help='ç”Ÿæˆç‰¹å®šæ¥å£çš„æ·±åº¦è°ƒç”¨é“¾æ ‘ï¼Œå¦‚ï¼š/user/user/login')
    
    # è§£ææ–¹æ³•é€‰æ‹©å‚æ•°
    parser.add_argument('--parse-method', choices=['regex', 'ast', 'jdt'], default='regex', 
                       help='é€‰æ‹©ä»£ç è§£ææ–¹æ³•: regex(æ­£åˆ™è¡¨è¾¾å¼,é»˜è®¤), ast(javalangè¯­æ³•æ ‘è§£æ) æˆ– jdt(Eclipse JDTç²¾ç¡®è§£æ,æ¨è)')
    parser.add_argument('--max-depth', type=int, default=4, 
                       help='æ·±åº¦è°ƒç”¨é“¾åˆ†æçš„æœ€å¤§æ·±åº¦ (é»˜è®¤: 4)')
    
    # è¿ç§»æ¨¡å¼å‚æ•°
    parser.add_argument('--old', help='æ—§é¡¹ç›®è·¯å¾„ï¼ˆè¿ç§»æ¨¡å¼å¿…éœ€ï¼‰')
    parser.add_argument('--new', help='æ–°é¡¹ç›®è·¯å¾„ï¼ˆè¿ç§»æ¨¡å¼å¿…éœ€ï¼‰')
    
    # é€šç”¨å‚æ•°
    parser.add_argument('--output', default='./migration_output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model', default='gpt-3.5-turbo', help='AIæ¨¡å‹åç§°')
    parser.add_argument('--api-key', help='AI APIå¯†é’¥ï¼Œæˆ–è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡')
    parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†åˆ†æä¿¡æ¯')
    parser.add_argument('--analyze-only', action='store_true', help='ä»…åˆ†æé¡¹ç›®ç»“æ„ï¼Œä¸ç”Ÿæˆè¿ç§»ä»£ç ')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.migrate:
        if not args.old or not args.new:
            parser.error("è¿ç§»æ¨¡å¼éœ€è¦åŒæ—¶æŒ‡å®š --old å’Œ --new å‚æ•°")
        
        config = Config(
            old_project_path=args.old,
            new_project_path=args.new,
            output_dir=args.output,
            ai_model=args.model,
            api_key=args.api_key,
            verbose=args.verbose,
            analyze_only=args.analyze_only or not (args.api_key or os.getenv("OPENAI_API_KEY")),
            single_mode=False
        )
    elif args.single:  # å•é¡¹ç›®æ¨¡å¼
        config = Config(
            single_project_path=args.single,
            output_dir=args.output,
            ai_model=args.model,
            api_key=args.api_key,
            verbose=args.verbose,
            analyze_only=True,  # å•é¡¹ç›®æ¨¡å¼é»˜è®¤åªåˆ†æ
            single_mode=True
        )
    elif args.show_endpoint:  # æ¥å£æŸ¥çœ‹æ¨¡å¼
        # ç›´æ¥è°ƒç”¨æ¥å£æŸ¥çœ‹åŠŸèƒ½ï¼Œä¸éœ€è¦åˆ›å»ºMigrationTool
        show_endpoint_details(args.show_endpoint, args.output)
        return
    else:  # è°ƒç”¨æ ‘ç”Ÿæˆæ¨¡å¼
        # ç›´æ¥è°ƒç”¨è°ƒç”¨æ ‘ç”ŸæˆåŠŸèƒ½
        generate_call_tree(args.call_tree, args.output, args.parse_method, args.max_depth)
        return
    
    # è¿è¡Œå·¥å…·
    tool = MigrationTool(config)
    tool.run()

if __name__ == "__main__":
    main()